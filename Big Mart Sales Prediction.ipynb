{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Mart Sales Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Problem Statement\n",
    "\n",
    "The data scientists at BigMart have collected sales data from 2013 for 1559 products across 10 stores in different cities. Along with this, certain attributes of each product and store have been defined. The goal of this data science project is to build a predictive model that estimates the sales of each product at a given store.\n",
    "\n",
    "- **Business Goal:**  \n",
    "  The objective is to use the model to understand the factors (properties of products and stores) that are key drivers in increasing sales.\n",
    "\n",
    "- **Problem Type:**  \n",
    "  This is a supervised learning problem.\n",
    "\n",
    "- **Target Feature:**  \n",
    "  The target variable for prediction is `Item_Outlet_Sales`.\n",
    "\n",
    "1. **Problem Statement**\n",
    "2. **Loading Packages and Data**\n",
    "3. **Exploratory Data Analysis (EDA)**\n",
    "4. **Label Encoding**\n",
    "5. **One-Hot Encoding**\n",
    "6. **Data Preprocessing**\n",
    "7. **Modeling**\n",
    "8. **Linear Regression**\n",
    "9. **Regularized Linear Regression**\n",
    "10. **Random Forest**\n",
    "11. **XGBoost**\n",
    "12. **Predictions & Summary**\n",
    "13. **Saving the Final Model**\n",
    "14. **Hyperparameter Tuning with RandomizedSearchCV**\n",
    "15. **Evaluating RandomizedSearchCV Tuned Models**\n",
    "16. **Final Predictions Using Best RandomizedSearchCV Model**\n",
    "17. **GridSearchCV for Hyperparameter Tuning**\n",
    "18. **Evaluating GridSearchCV Tuned Models**\n",
    "19. **Final Predictions Using Best GridSearchCV Model**\n",
    "20. **Saving the Final GridSearchCV Model Predictions**\n",
    "\n",
    "---\n",
    "\n",
    "## Data Dictionary\n",
    "\n",
    "We have two datasets: **train** and **test**. The **train dataset** contains both input and output variables, while the **test dataset** only includes input variables for which sales need to be predicted.\n",
    "\n",
    "### Train Dataset (8523 rows)\n",
    "\n",
    "| Variable                  | Description                                                              |\n",
    "|---------------------------|--------------------------------------------------------------------------|\n",
    "| `Item_Identifier`          | Unique product ID                                                        |\n",
    "| `Item_Weight`              | Weight of the product                                                     |\n",
    "| `Item_Fat_Content`         | Whether the product is low fat or not                                    |\n",
    "| `Item_Visibility`          | Percentage of total display area allocated to the product in a store    |\n",
    "| `Item_Type`                | The category the product belongs to                                      |\n",
    "| `Item_MRP`                 | Maximum Retail Price (list price) of the product                        |\n",
    "| `Outlet_Identifier`        | Unique store ID                                                          |\n",
    "| `Outlet_Establishment_Year`| The year in which the store was established                              |\n",
    "| `Outlet_Size`              | The size of the store in terms of ground area covered                    |\n",
    "| `Outlet_Location_Type`     | The type of city in which the store is located                           |\n",
    "| `Outlet_Type`              | Whether the outlet is a grocery store or a supermarket                   |\n",
    "| `Item_Outlet_Sales`        | Sales of the product in the store (target variable)                     |\n",
    "\n",
    "### Test Dataset (5681 rows)\n",
    "\n",
    "| Variable                  | Description                                                              |\n",
    "|---------------------------|--------------------------------------------------------------------------|\n",
    "| `Item_Identifier`          | Unique product ID                                                        |\n",
    "| `Item_Weight`              | Weight of the product                                                     |\n",
    "| `Item_Fat_Content`         | Whether the product is low fat or not                                    |\n",
    "| `Item_Visibility`          | Percentage of total display area allocated to the product in a store    |\n",
    "| `Item_Type`                | The category the product belongs to                                      |\n",
    "| `Item_MRP`                 | Maximum Retail Price (list price) of the product                        |\n",
    "| `Outlet_Identifier`        | Unique store ID                                                          |\n",
    "| `Outlet_Establishment_Year`| The year in which the store was established                              |\n",
    "| `Outlet_Size`              | The size of the store in terms of ground area covered                    |\n",
    "| `Outlet_Location_Type`     | The type of city in which the store is located                           |\n",
    "| `Outlet_Type`              | Whether the outlet is a grocery store or a supermarket                   |\n",
    "\n",
    "---\n",
    "\n",
    "## Submission File Format\n",
    "\n",
    "To submit your predictions, the output file should be structured as follows:\n",
    "\n",
    "| Variable               | Description                                                   |\n",
    "|------------------------|---------------------------------------------------------------|\n",
    "| `Item_Identifier`       | Unique product ID                                             |\n",
    "| `Outlet_Identifier`     | Unique store ID                                               |\n",
    "| `Item_Outlet_Sales`     | Predicted sales of the product in the particular store       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Packages and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Load datasets\n",
    "train = pd.read_csv(r'H:\\MY WORK SPACE\\PROJECT GITHUB\\NOTES\\ML\\AV\\train.csv')\n",
    "test = pd.read_csv(r'H:\\MY WORK SPACE\\PROJECT GITHUB\\NOTES\\ML\\AV\\test.csv')\n",
    "\n",
    "# Save the original identifiers before processing\n",
    "original_test_identifiers = test[['Item_Identifier', 'Outlet_Identifier']].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item_Identifier</th>\n",
       "      <th>Item_Weight</th>\n",
       "      <th>Item_Fat_Content</th>\n",
       "      <th>Item_Visibility</th>\n",
       "      <th>Item_Type</th>\n",
       "      <th>Item_MRP</th>\n",
       "      <th>Outlet_Identifier</th>\n",
       "      <th>Outlet_Establishment_Year</th>\n",
       "      <th>Outlet_Size</th>\n",
       "      <th>Outlet_Location_Type</th>\n",
       "      <th>Outlet_Type</th>\n",
       "      <th>Item_Outlet_Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FDA15</td>\n",
       "      <td>9.30</td>\n",
       "      <td>Low Fat</td>\n",
       "      <td>0.016047</td>\n",
       "      <td>Dairy</td>\n",
       "      <td>249.8092</td>\n",
       "      <td>OUT049</td>\n",
       "      <td>1999</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Tier 1</td>\n",
       "      <td>Supermarket Type1</td>\n",
       "      <td>3735.1380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DRC01</td>\n",
       "      <td>5.92</td>\n",
       "      <td>Regular</td>\n",
       "      <td>0.019278</td>\n",
       "      <td>Soft Drinks</td>\n",
       "      <td>48.2692</td>\n",
       "      <td>OUT018</td>\n",
       "      <td>2009</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Tier 3</td>\n",
       "      <td>Supermarket Type2</td>\n",
       "      <td>443.4228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FDN15</td>\n",
       "      <td>17.50</td>\n",
       "      <td>Low Fat</td>\n",
       "      <td>0.016760</td>\n",
       "      <td>Meat</td>\n",
       "      <td>141.6180</td>\n",
       "      <td>OUT049</td>\n",
       "      <td>1999</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Tier 1</td>\n",
       "      <td>Supermarket Type1</td>\n",
       "      <td>2097.2700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FDX07</td>\n",
       "      <td>19.20</td>\n",
       "      <td>Regular</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Fruits and Vegetables</td>\n",
       "      <td>182.0950</td>\n",
       "      <td>OUT010</td>\n",
       "      <td>1998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tier 3</td>\n",
       "      <td>Grocery Store</td>\n",
       "      <td>732.3800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NCD19</td>\n",
       "      <td>8.93</td>\n",
       "      <td>Low Fat</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Household</td>\n",
       "      <td>53.8614</td>\n",
       "      <td>OUT013</td>\n",
       "      <td>1987</td>\n",
       "      <td>High</td>\n",
       "      <td>Tier 3</td>\n",
       "      <td>Supermarket Type1</td>\n",
       "      <td>994.7052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Item_Identifier  Item_Weight Item_Fat_Content  Item_Visibility  \\\n",
       "0           FDA15         9.30          Low Fat         0.016047   \n",
       "1           DRC01         5.92          Regular         0.019278   \n",
       "2           FDN15        17.50          Low Fat         0.016760   \n",
       "3           FDX07        19.20          Regular         0.000000   \n",
       "4           NCD19         8.93          Low Fat         0.000000   \n",
       "\n",
       "               Item_Type  Item_MRP Outlet_Identifier  \\\n",
       "0                  Dairy  249.8092            OUT049   \n",
       "1            Soft Drinks   48.2692            OUT018   \n",
       "2                   Meat  141.6180            OUT049   \n",
       "3  Fruits and Vegetables  182.0950            OUT010   \n",
       "4              Household   53.8614            OUT013   \n",
       "\n",
       "   Outlet_Establishment_Year Outlet_Size Outlet_Location_Type  \\\n",
       "0                       1999      Medium               Tier 1   \n",
       "1                       2009      Medium               Tier 3   \n",
       "2                       1999      Medium               Tier 1   \n",
       "3                       1998         NaN               Tier 3   \n",
       "4                       1987        High               Tier 3   \n",
       "\n",
       "         Outlet_Type  Item_Outlet_Sales  \n",
       "0  Supermarket Type1          3735.1380  \n",
       "1  Supermarket Type2           443.4228  \n",
       "2  Supermarket Type1          2097.2700  \n",
       "3      Grocery Store           732.3800  \n",
       "4  Supermarket Type1           994.7052  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display first few rows of the training data\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Handling Missing Values and Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "train['Outlet_Size'] = train['Outlet_Size'].fillna(train['Outlet_Size'].mode()[0])\n",
    "train['Item_Weight'] = train['Item_Weight'].fillna(train['Item_Weight'].mean())\n",
    "test['Outlet_Size'] = test['Outlet_Size'].fillna(test['Outlet_Size'].mode()[0])\n",
    "test['Item_Weight'] = test['Item_Weight'].fillna(test['Item_Weight'].mean())\n",
    "\n",
    "# Clean Item_Fat_Content\n",
    "train['Item_Fat_Content'] = train['Item_Fat_Content'].replace({'low fat': 'Low Fat', 'LF': 'Low Fat', 'reg': 'Regular'})\n",
    "test['Item_Fat_Content'] = test['Item_Fat_Content'].replace({'low fat': 'Low Fat', 'LF': 'Low Fat', 'reg': 'Regular'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Outlet_Age feature\n",
    "train['Outlet_Age'] = 2023 - train['Outlet_Establishment_Year']\n",
    "test['Outlet_Age'] = 2023 - test['Outlet_Establishment_Year']\n",
    "train.drop('Outlet_Establishment_Year', axis=1, inplace=True)\n",
    "test.drop('Outlet_Establishment_Year', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Encode Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "train['Outlet_Size'] = train['Outlet_Size'].map({'Small': 1, 'Medium': 2, 'High': 3}).astype(int)\n",
    "test['Outlet_Size'] = test['Outlet_Size'].map({'Small': 1, 'Medium': 2, 'High': 3}).astype(int)\n",
    "\n",
    "train['Outlet_Location_Type'] = train['Outlet_Location_Type'].str[-1:].astype(int)\n",
    "test['Outlet_Location_Type'] = test['Outlet_Location_Type'].str[-1:].astype(int)\n",
    "\n",
    "# Encode 'Item_Identifier' categories\n",
    "train['Item_Identifier_Categories'] = train['Item_Identifier'].str[0:2]\n",
    "test['Item_Identifier_Categories'] = test['Item_Identifier'].str[0:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Label Encoding for Ordinal Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding for ordinal columns\n",
    "encoder = LabelEncoder()\n",
    "ordinal_features = ['Item_Fat_Content', 'Outlet_Type', 'Outlet_Location_Type']\n",
    "for feature in ordinal_features:\n",
    "    train[feature] = encoder.fit_transform(train[feature])\n",
    "    test[feature] = encoder.transform(test[feature])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding\n",
    "train = pd.get_dummies(train, columns=['Item_Type', 'Item_Identifier_Categories', 'Outlet_Identifier'], drop_first=True)\n",
    "test = pd.get_dummies(test, columns=['Item_Type', 'Item_Identifier_Categories', 'Outlet_Identifier'], drop_first=True)\n",
    "\n",
    "# Drop 'Item_Identifier' as it's encoded, but keep the original identifiers for final output\n",
    "train.drop(labels=['Item_Identifier'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Aligning Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align the test DataFrame with the training DataFrame\n",
    "missing_cols = set(train.columns) - set(test.columns)\n",
    "for col in missing_cols:\n",
    "    test[col] = 0  # Add missing columns with default value 0\n",
    "\n",
    "# Ensure the columns are in the same order\n",
    "test = test[train.columns.drop('Item_Outlet_Sales')]  # Drop target column if it exists\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Split Data into Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "X = train.drop('Item_Outlet_Sales', axis=1)\n",
    "y = train['Item_Outlet_Sales']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model Definition and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.243e+08, tolerance: 2.017e+06\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance (R² scores):\n",
      "LinearRegression: 0.6094\n",
      "Ridge: 0.6096\n",
      "Lasso: 0.6103\n",
      "RandomForest: 0.5615\n",
      "XGBoost: 0.5299\n"
     ]
    }
   ],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    \"LinearRegression\": Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('poly', PolynomialFeatures(degree=2)),\n",
    "        ('model', LinearRegression())\n",
    "    ]),\n",
    "    \"Ridge\": Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('poly', PolynomialFeatures(degree=2)),\n",
    "        ('model', Ridge(alpha=7, fit_intercept=True))\n",
    "    ]),\n",
    "    \"Lasso\": Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('poly', PolynomialFeatures(degree=2)),\n",
    "        ('model', Lasso(alpha=0.2, fit_intercept=True))\n",
    "    ]),\n",
    "    \"RandomForest\": RandomForestRegressor(),\n",
    "    \"XGBoost\": XGBRegressor()\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "best_model = None\n",
    "best_score = float('-inf')\n",
    "model_performance = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    model_performance[name] = r2\n",
    "    \n",
    "    if r2 > best_score:\n",
    "        best_score = r2\n",
    "        best_model = model\n",
    "\n",
    "# Print model performances\n",
    "print(\"Model Performance (R² scores):\")\n",
    "for model, score in model_performance.items():\n",
    "    print(f\"{model}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Making Final Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Item_Identifier Outlet_Identifier  Item_Outlet_Sales\n",
      "0           FDW58            OUT049        1621.924407\n",
      "1           FDW14            OUT017        1634.367658\n",
      "2           NCN55            OUT010         553.055585\n",
      "3           FDQ58            OUT017        2631.382221\n",
      "4           FDY38            OUT027        5808.189893\n"
     ]
    }
   ],
   "source": [
    "# Use the best model for final predictions\n",
    "final_predictions = best_model.predict(test)\n",
    "\n",
    "# Ensure no negative sales values\n",
    "final_predictions = np.maximum(final_predictions, 0)\n",
    "\n",
    "# Format the final output\n",
    "final_output = original_test_identifiers.copy()  # Use the original identifiers\n",
    "final_output['Item_Outlet_Sales'] = final_predictions\n",
    "\n",
    "# Save to CSV in the requested format\n",
    "final_output.to_csv('final_prediction.csv', index=False)\n",
    "\n",
    "# Display first few rows of the output\n",
    "print(final_output.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Hyperparameter Tuning (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Define hyperparameter distributions for each model (using a range of values)\n",
    "param_distributions = {\n",
    "    'Ridge': {\n",
    "        'model__alpha': [0.1, 1, 10, 100]\n",
    "    },\n",
    "    'Lasso': {\n",
    "        'model__alpha': [0.1, 0.2, 0.5, 1.0]\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'n_estimators': [50, 100, 150, 200],\n",
    "        'max_depth': [5, 10, 15, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 6, 9],\n",
    "        'subsample': [0.8, 0.9, 1.0]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Randomized Search CV for Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Ridge...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:317: UserWarning: The total space of parameters 4 is smaller than n_iter=10. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Ridge: {'model__alpha': 100}\n",
      "Tuning Lasso...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:317: UserWarning: The total space of parameters 4 is smaller than n_iter=10. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "c:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.612e+07, tolerance: 2.017e+06\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Lasso: {'model__alpha': 1.0}\n",
      "Tuning RandomForest...\n",
      "Best parameters for RandomForest: {'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_depth': 5}\n"
     ]
    }
   ],
   "source": [
    "# Update models with a more generalized pipeline for RandomizedSearchCV (excluding XGBoost)\n",
    "models_tuned = {\n",
    "    \"Ridge\": Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('poly', PolynomialFeatures(degree=2)),\n",
    "        ('model', Ridge(fit_intercept=True))\n",
    "    ]),\n",
    "    \"Lasso\": Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('poly', PolynomialFeatures(degree=2)),\n",
    "        ('model', Lasso(fit_intercept=True))\n",
    "    ]),\n",
    "    \"RandomForest\": RandomForestRegressor(),\n",
    "}\n",
    "\n",
    "# Perform RandomizedSearchCV for hyperparameter tuning with cross-validation (excluding XGBoost)\n",
    "best_models = {}\n",
    "for name, model in models_tuned.items():\n",
    "    print(f\"Tuning {name}...\")\n",
    "    random_search = RandomizedSearchCV(estimator=model, param_distributions=param_distributions[name], \n",
    "                                       n_iter=10, cv=5, n_jobs=-1, scoring='neg_mean_squared_error', random_state=42)\n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_models[name] = random_search.best_estimator_\n",
    "    print(f\"Best parameters for {name}: {random_search.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. XGBoost Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning XGBoost...\n",
      "Best parameters for XGBoost: {'subsample': 0.9, 'n_estimators': 50, 'max_depth': 3, 'learning_rate': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# Separate XGBoost tuning using XGBoost's built-in RandomizedSearchCV\n",
    "print(f\"Tuning XGBoost...\")\n",
    "\n",
    "xgb_model = XGBRegressor()\n",
    "\n",
    "# Perform RandomizedSearchCV with XGBoost's hyperparameters\n",
    "xgb_random_search = RandomizedSearchCV(estimator=xgb_model, param_distributions=param_distributions['XGBoost'],\n",
    "                                       n_iter=10, cv=5, n_jobs=-1, scoring='neg_mean_squared_error', random_state=42)\n",
    "xgb_random_search.fit(X_train, y_train)\n",
    "\n",
    "best_models['XGBoost'] = xgb_random_search.best_estimator_\n",
    "print(f\"Best parameters for XGBoost: {xgb_random_search.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Evaluate the Tuned Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.612e+07, tolerance: 2.017e+06\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tuned Model Performance:\n",
      "Ridge: R² = 0.6110, MSE = 1057249.2994, RMSE = 1028.2263\n",
      "Lasso: R² = 0.6123, MSE = 1053805.9981, RMSE = 1026.5505\n",
      "RandomForest: R² = 0.6173, MSE = 1040154.5184, RMSE = 1019.8797\n",
      "XGBoost: R² = 0.6138, MSE = 1049633.3852, RMSE = 1024.5162\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Evaluate the tuned models\n",
    "model_performance = {}\n",
    "for name, model in best_models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    model_performance[name] = {\n",
    "        'R²': r2,\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse\n",
    "    }\n",
    "\n",
    "# Print performance of the tuned models\n",
    "print(\"\\nTuned Model Performance:\")\n",
    "for model, metrics in model_performance.items():\n",
    "    print(f\"{model}: R² = {metrics['R²']:.4f}, MSE = {metrics['MSE']:.4f}, RMSE = {metrics['RMSE']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Final Predictions Using Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Model: RandomForest with R² = 0.6173\n",
      "\n",
      "Final Prediction Output:\n",
      "  Item_Identifier Outlet_Identifier  Item_Outlet_Sales\n",
      "0           FDW58            OUT049        1583.792881\n",
      "1           FDW14            OUT017        1448.255155\n",
      "2           NCN55            OUT010         552.911318\n",
      "3           FDQ58            OUT017        2466.141009\n",
      "4           FDY38            OUT027        6243.348774\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Select the best model based on R²\n",
    "best_model = max(model_performance, key=lambda x: model_performance[x]['R²'])\n",
    "print(f\"\\nBest Model: {best_model} with R² = {model_performance[best_model]['R²']:.4f}\")\n",
    "\n",
    "# Use the best model for final predictions\n",
    "final_predictions = best_models[best_model].predict(test)\n",
    "\n",
    "# Ensure no negative sales values\n",
    "final_predictions = np.maximum(final_predictions, 0)\n",
    "\n",
    "# Format the final output\n",
    "final_output = original_test_identifiers.copy()  # Use the original identifiers\n",
    "final_output['Item_Outlet_Sales'] = final_predictions\n",
    "\n",
    "# Save to CSV in the requested format\n",
    "final_output.to_csv('final_prediction_randomized_tuned.csv', index=False)\n",
    "\n",
    "# Display first few rows of the output\n",
    "print(\"\\nFinal Prediction Output:\")\n",
    "print(final_output.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.GridSearchCV for Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Ridge using GridSearchCV...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Best parameters for Ridge: {'model__alpha': 100}\n",
      "Tuning Lasso using GridSearchCV...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.612e+07, tolerance: 2.017e+06\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Lasso: {'model__alpha': 1.0}\n",
      "Tuning RandomForest using GridSearchCV...\n",
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n",
      "Best parameters for RandomForest: {'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 150}\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the hyperparameter grid for each model\n",
    "param_grid = {\n",
    "    'Ridge': {\n",
    "        'model__alpha': [0.1, 1, 10, 100]\n",
    "    },\n",
    "    'Lasso': {\n",
    "        'model__alpha': [0.1, 0.2, 0.5, 1.0]\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'n_estimators': [50, 100, 150, 200],\n",
    "        'max_depth': [5, 10, 15, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 6, 9],\n",
    "        'subsample': [0.8, 0.9, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV for hyperparameter tuning with cross-validation\n",
    "best_grid_models = {}\n",
    "for name, model in models_tuned.items():\n",
    "    print(f\"Tuning {name} using GridSearchCV...\")\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid[name], \n",
    "                               cv=5, n_jobs=-1, scoring='neg_mean_squared_error', verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_grid_models[name] = grid_search.best_estimator_\n",
    "    print(f\"Best parameters for {name}: {grid_search.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19. Evaluate the GridSearchCV Tuned Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.612e+07, tolerance: 2.017e+06\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GridSearchCV Tuned Model Performance:\n",
      "Ridge: R² = 0.6110, MSE = 1057249.2994, RMSE = 1028.2263\n",
      "Lasso: R² = 0.6123, MSE = 1053805.9981, RMSE = 1026.5505\n",
      "RandomForest: R² = 0.6174, MSE = 1039783.3210, RMSE = 1019.6977\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Evaluate the models after GridSearchCV tuning\n",
    "grid_model_performance = {}\n",
    "for name, model in best_grid_models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    grid_model_performance[name] = {\n",
    "        'R²': r2,\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse\n",
    "    }\n",
    "\n",
    "# Print performance of the GridSearchCV tuned models\n",
    "print(\"\\nGridSearchCV Tuned Model Performance:\")\n",
    "for model, metrics in grid_model_performance.items():\n",
    "    print(f\"{model}: R² = {metrics['R²']:.4f}, MSE = {metrics['MSE']:.4f}, RMSE = {metrics['RMSE']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20. Final Predictions Using Best GridSearchCV Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best GridSearchCV Model: RandomForest with R² = 0.6174\n",
      "\n",
      "Final Prediction Output with GridSearchCV:\n",
      "  Item_Identifier Outlet_Identifier  Item_Outlet_Sales\n",
      "0           FDW58            OUT049        1608.551442\n",
      "1           FDW14            OUT017        1436.188902\n",
      "2           NCN55            OUT010         551.496070\n",
      "3           FDQ58            OUT017        2492.406512\n",
      "4           FDY38            OUT027        6294.357435\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Select the best model from GridSearchCV based on R²\n",
    "best_grid_model = max(grid_model_performance, key=lambda x: grid_model_performance[x]['R²'])\n",
    "print(f\"\\nBest GridSearchCV Model: {best_grid_model} with R² = {grid_model_performance[best_grid_model]['R²']:.4f}\")\n",
    "\n",
    "# Use the best model for final predictions\n",
    "final_predictions_grid = best_grid_models[best_grid_model].predict(test)\n",
    "\n",
    "# Ensure no negative sales values\n",
    "final_predictions_grid = np.maximum(final_predictions_grid, 0)\n",
    "\n",
    "# Format the final output\n",
    "final_output_grid = original_test_identifiers.copy()  # Use the original identifiers\n",
    "final_output_grid['Item_Outlet_Sales'] = final_predictions_grid\n",
    "\n",
    "# Save to CSV in the requested format\n",
    "final_output_grid.to_csv('final_prediction_grid_search.csv', index=False)\n",
    "\n",
    "# Display first few rows of the output\n",
    "print(\"\\nFinal Prediction Output with GridSearchCV:\")\n",
    "print(final_output_grid.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
